{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 : Prompt Engineering for Large Language Models (LLMs) [4 marks]\n",
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (126, 500, 3)\n",
      "Testing data shape:  (54, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "#\n",
    "#                                   ES335- Machine Learning- Assignment 1\n",
    "#\n",
    "# This file is used to create the dataset for the mini-project. The dataset is created by reading the data from\n",
    "# the Combined folder. The data is then split into training, testing, and validation sets. This split is supposed\n",
    "# to be used for all the modeling purposes.\n",
    "#\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# Constants\n",
    "time = 10\n",
    "offset = 100\n",
    "folders = [\"LAYING\", \"SITTING\", \"STANDING\", \"WALKING\", \"WALKING_DOWNSTAIRS\", \"WALKING_UPSTAIRS\"]\n",
    "classes = {\"WALKING\": 1, \"WALKING_UPSTAIRS\": 2, \"WALKING_DOWNSTAIRS\": 3, \"SITTING\": 4, \"STANDING\": 5, \"LAYING\": 6}\n",
    "\n",
    "combined_dir = os.path.join(\"Combined\")\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Train Dataset\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "dataset_dir = os.path.join(combined_dir, \"Train\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir, folder))\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, folder, file), sep=\",\", header=0)\n",
    "        df = df[offset:offset + time * 50]\n",
    "        X_train.append(df.values)\n",
    "        y_train.append(classes[folder])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Test Dataset\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "dataset_dir = os.path.join(combined_dir, \"Test\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir, folder))\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, folder, file), sep=\",\", header=0)\n",
    "        df = df[offset:offset + time * 50]\n",
    "        X_test.append(df.values)\n",
    "        y_test.append(classes[folder])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Final Dataset\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# USE THE BELOW GIVEN DATA FOR TRAINING and TESTING purposes\n",
    "\n",
    "# concatenate the training and testing data\n",
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "\n",
    "# split the data into training and testing sets. Change the seed value to obtain different random splits.\n",
    "seed = 4\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "\n",
    "print(\"Training data shape: \", X_train.shape)\n",
    "print(\"Testing data shape: \", X_test.shape)\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataframe shape: (63000, 5)\n",
      "Testing Datafrane shape: (27000, 5)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(np.concatenate((X_train.reshape((-1, 3)), np.repeat(y_train, 500).reshape(-1,1)), axis = 1), columns = [\"accx\", \"accy\", \"accz\", \"Label\"])\n",
    "train_df[\"totalacc\"] = train_df[\"accx\"]**2 + train_df[\"accy\"]**2 + train_df[\"accz\"]**2\n",
    "print(\"Training Dataframe shape:\", train_df.shape)\n",
    "\n",
    "test_df = pd.DataFrame(np.concatenate((X_test.reshape((-1, 3)), np.repeat(y_test, 500).reshape(-1,1)), axis = 1), columns = [\"accx\", \"accy\", \"accz\", \"Label\"])\n",
    "test_df[\"totalacc\"] = test_df[\"accx\"]**2 + test_df[\"accy\"]**2 + test_df[\"accz\"]**2\n",
    "print(\"Testing Datafrane shape:\", test_df.shape)\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# Label wise distribution of the data\n",
    "train_df_class_1 = train_df[train_df[\"Label\"] == 1]\n",
    "train_df_class_2 = train_df[train_df[\"Label\"] == 2]\n",
    "train_df_class_3 = train_df[train_df[\"Label\"] == 3]\n",
    "train_df_class_4 = train_df[train_df[\"Label\"] == 4]\n",
    "train_df_class_5 = train_df[train_df[\"Label\"] == 5]\n",
    "train_df_class_6 = train_df[train_df[\"Label\"] == 6]\n",
    "\n",
    "test_df_class_1 = test_df[test_df[\"Label\"] == 1]\n",
    "test_df_class_2 = test_df[test_df[\"Label\"] == 2]\n",
    "test_df_class_3 = test_df[test_df[\"Label\"] == 3]\n",
    "test_df_class_4 = test_df[test_df[\"Label\"] == 4]\n",
    "test_df_class_5 = test_df[test_df[\"Label\"] == 5]\n",
    "test_df_class_6 = test_df[test_df[\"Label\"] == 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model and API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq API and Models \n",
    "load_dotenv()\n",
    "groq_api_key_akash = os.getenv(\"API_KEY_AKASH\") #loading api key from .env file \n",
    "groq_api_key_romit= os.getenv(\"API_KEY_ROMIT\")\n",
    "\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\", \n",
    "    \"mixtral\": \"mixtral-8x7b-32768\", \n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\":\"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\":\"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\":\"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\":\"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "modelName = \"llama3-70b\"\n",
    "\n",
    "llm = ChatGroq(model=groq_models[modelName], api_key=groq_api_key_romit, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Demonstrate how to use Zero-Shot Learning and Few-Shot Learning to classify human activities based on the featurized accelerometer data. Qualitatively demonstrate the performance of Few-Shot Learning with Zero-Shot Learning. Which method performs better? Why? [1 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both `Zero Shot Learning` and `Few Shot Learning`, we'll be using the `test_df` to get classification answers and tell accuracy.\n",
    "\n",
    "We'll pass data to the llm (for both training in case of Few-Shot and testing, in Windows of 5 seconds after averaging over non-overlapping 0.2 second windows)\n",
    "- Zero Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_window_width = 5 # in seconds\n",
    "ind_window_width = 0.2 # in seconds\n",
    "sampling_rate = 50 #in Hertz\n",
    "num_of_windows = total_window_width / ind_window_width\n",
    "\n",
    "# function to get test data timeseries from testing dataset\n",
    "def get_avg_acc_data(start_seconds, activity_class):\n",
    "    match activity_class:\n",
    "        case 1:\n",
    "            df = test_df_class_1\n",
    "        case 2:\n",
    "            df = test_df_class_2\n",
    "        case 3:\n",
    "            df = test_df_class_3\n",
    "        case 4:\n",
    "            df = test_df_class_4\n",
    "        case 5:\n",
    "            df = test_df_class_5\n",
    "        case 6:\n",
    "            df = test_df_class_6\n",
    "    # Get the average data from the dataframe\n",
    "    # print(df.head())\n",
    "    five_sec_data = df[[\"accx\", \"accy\", \"accz\"]][start_seconds * sampling_rate:(start_seconds + 5) * sampling_rate]\n",
    "    avg_data = [np.mean(window, axis=0).to_list() for window in np.array_split(five_sec_data, num_of_windows)]\n",
    "\n",
    "    #plotting the generated data\n",
    "    # plt.plot(np.arange(start_seconds, start_seconds + total_window_width, ind_window_width), [data[0] for data in avg_data], label=\"x\")\n",
    "    # plt.plot(np.arange(start_seconds, start_seconds + total_window_width, ind_window_width), [data[1] for data in avg_data], label=\"y\")\n",
    "    # plt.plot(np.arange(start_seconds, start_seconds + total_window_width, ind_window_width), [data[2] for data in avg_data], label=\"z\")\n",
    "    # plt.legend()\n",
    "    # plt.ylim(-1, 2)\n",
    "    # plt.show()\n",
    "\n",
    "    return avg_data\n",
    "\n",
    "def build_query_zeroshot(string):\n",
    "    return f\"\"\"\n",
    "    * You are a Human Activity Recognition model. \n",
    "    * Your task is to classify the given acceleration data in terms of g (acceleration at earth's surface), which is data of a five second window after averaging over 0.5 second non-overlapping windows, resulting in 10 data points.\n",
    "    * The data is recorded by accelerometer positioned above the torso of the person.\n",
    "    * The data's format is as such that each line has acceleration in X, acceleration in Y, acceleration in Z respectively where X is the direction along the person's height, Y is along the person's breadth seen from front and Z is perpendicular to the person's body. \n",
    "    * Also note that the magnitude of the acceleration values take into account the earth's gravity, showing which axis is pointing towards the earth's center and not necessarily showing if the person is accelerating in that direction.\n",
    "    * Pay particular attention to the variation in the data over the given window to differentiate between static and dynamic activities. Also pay attention to which axis has higher value, indicating the person's orientation\n",
    "    1. Walking\n",
    "    2. Walking Upstairs\n",
    "    3. Walking Downstairs\n",
    "    4. Sitting\n",
    "    5. Standing\n",
    "    6. Lying Down\n",
    "\n",
    "    * Provide JUST the number corresponding to the predicted activity\n",
    "\n",
    "    data: \n",
    "    {string}\n",
    "    \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('WALKING', 1), ('WALKING_UPSTAIRS', 2), ('WALKING_DOWNSTAIRS', 3), ('SITTING', 4), ('STANDING', 5), ('LAYING', 6)]\n",
      "Max Accuracy over 5 runs of 20 prompts each: 30.0 \n",
      "Mean: 20.0\n",
      "Max Accuracy for just differentiating between static and dynamic activities: 80.0 \n",
      "Mean: 69.0\n"
     ]
    }
   ],
   "source": [
    "m_iters = 5\n",
    "iters = 20\n",
    "\n",
    "def evaluate_llm_zeroshot(iters = 10):\n",
    "    runs_results = []\n",
    "    runs_results_staticdynamic = []\n",
    "    for _ in range(iters):\n",
    "        crct_class = random.randint(1, 6)\n",
    "        start_time = random.randint(0, 5)\n",
    "        data = get_avg_acc_data(start_time, crct_class)\n",
    "        data_string = \"\\n\".join([str(point) for point in data])\n",
    "        query = build_query_zeroshot(data_string)\n",
    "        \n",
    "        answer = llm.invoke(query)\n",
    "\n",
    "        # print(answer.content, crct_class)\n",
    "        runs_results.append(1 if int(answer.content.split()[-1].lstrip(\"(\").rstrip(\").\")) == crct_class else 0)\n",
    "        runs_results_staticdynamic.append(1 if (int(answer.content.split()[-1].lstrip(\"(\").rstrip(\").\")) in [1, 2, 3] and crct_class in [1, 2, 3]) or (int(answer.content.split()[-1].lstrip(\"(\").rstrip(\").\")) in [4, 5, 6] and crct_class in [4, 5, 6]) else 0)\n",
    "    \n",
    "    return (np.round(np.mean(runs_results), 4), np.round(np.mean(runs_results_staticdynamic), 4))\n",
    "\n",
    "\n",
    "print(list(classes.items()))\n",
    "accuracies = []\n",
    "accuracies_staticdynamic = []\n",
    "for _ in range(m_iters):\n",
    "    accuracy = evaluate_llm_zeroshot(iters=iters)\n",
    "    accuracies.append(accuracy[0])\n",
    "    accuracies_staticdynamic.append(accuracy[1])\n",
    "\n",
    "print(f\"Max Accuracy over {m_iters} runs of {iters} prompts each:\", max(accuracies) * 100, \"\\nMean:\", np.round(np.mean(accuracies) * 100, 2))\n",
    "print(\"Max Accuracy for just differentiating between static and dynamic activities:\", max(accuracies_staticdynamic) * 100, \"\\nMean:\", np.round(np.mean(accuracies_staticdynamic) * 100, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Few Shot Learning\n",
    "\n",
    "We provide the example data (from training dataset) along with their correct labels for each class of activity to the model first, then we ask it to classify a timeseries data from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITIES = {\n",
    "    1: 'WALKING',\n",
    "    2: 'WALKING_UPSTAIRS',\n",
    "    3: 'WALKING_DOWNSTAIRS',\n",
    "    4: 'SITTING',\n",
    "    5: 'STANDING',\n",
    "    6: 'LAYING',\n",
    "}\n",
    "\n",
    "# function to get example data timeseries for prompt - from training dataset\n",
    "def get_avg_acc_train_data(start_seconds, activity_class):\n",
    "    match activity_class:\n",
    "        case 1:\n",
    "            df = train_df_class_1\n",
    "        case 2:\n",
    "            df = train_df_class_2\n",
    "        case 3:\n",
    "            df = train_df_class_3\n",
    "        case 4:\n",
    "            df = train_df_class_4\n",
    "        case 5:\n",
    "            df = train_df_class_5\n",
    "        case 6:\n",
    "            df = train_df_class_6\n",
    "    # Get the average data from the dataframe\n",
    "    # print(df.head())\n",
    "    five_sec_data = df[[\"accx\", \"accy\", \"accz\"]][start_seconds * sampling_rate:(start_seconds + 5) * sampling_rate]\n",
    "    avg_data = [np.mean(window, axis=0).to_list() for window in np.array_split(five_sec_data, num_of_windows)]\n",
    "\n",
    "    return avg_data\n",
    "\n",
    "def build_query_fewshot(train, test):\n",
    "    return f\"\"\"\n",
    "    * You are a Human Activity Recognition model. \n",
    "    * Your task is to classify the given acceleration data in terms of g (acceleration at earth's surface), which is data of a 5 second window after averaging over 0.5 second non-overlapping windows, resulting in 10 data points.\n",
    "    * The data is recorded by accelerometer positioned above the torso of the person.\n",
    "    * The data's format is as such that each line has a datapoint containing acceleration in X, acceleration in Y, acceleration in Z respectively where X is the direction along the person's height, Y is along the person's breadth seen from front and Z is perpendicular to the person's body. \n",
    "    * Also note that the magnitude of the acceleration values take into account the earth's gravity, showing which axis is pointing towards the earth's center and not necessarily showing if the person is accelerating in that direction. This can be helpful in determining the orientation of the person's body. \n",
    "    * Pay particular attention to the variation in the data over the given window to differentiate between static and dynamic activities. Also pay attention to which axis has higher value, indicating the person's orientation.\n",
    "\n",
    "    1. Walking\n",
    "    2. Walking Upstairs\n",
    "    3. Walking Downstairs\n",
    "    4. Sitting\n",
    "    5. Standing\n",
    "    6. Lying Down\n",
    "\n",
    "    *Here are few examples:\n",
    "    {train}\n",
    "\n",
    "    * Provide just the number corresponding to the predicted activity\n",
    "\n",
    "    data: \n",
    "    {test}\n",
    "    \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('WALKING', 1), ('WALKING_UPSTAIRS', 2), ('WALKING_DOWNSTAIRS', 3), ('SITTING', 4), ('STANDING', 5), ('LAYING', 6)]\n"
     ]
    }
   ],
   "source": [
    "number_of_examples = 6\n",
    "def evaluate_llm_fewshot(iters = 10):\n",
    "    runs_results = []\n",
    "    runs_results_staticdynamic = []\n",
    "    for _ in range(iters):\n",
    "        crct_class = random.randint(1, 6)\n",
    "        start_time = random.randint(0, 5)\n",
    "\n",
    "        train_string = \"\"\n",
    "        for i in range(number_of_examples):\n",
    "            c = (i % 6) + 1\n",
    "            train_string += str(i+1) + \" Timeseries data:\\n\" + \"\\n\".join([str(point) for point in get_avg_acc_train_data(start_time, c)]) + \"\\nActivity: \" + str(c) + \": \" + ACTIVITIES[c] + \"\\n\\n\"\n",
    "\n",
    "        test_data = get_avg_acc_data(start_time, crct_class)\n",
    "        test_string = \"\\n\".join([str(point) for point in test_data])\n",
    "        query = build_query_fewshot(train_string, test_string)\n",
    "        # print(query)\n",
    "        \n",
    "        answer = llm.invoke(query)\n",
    "\n",
    "        # print(answer.content, crct_class)\n",
    "        runs_results.append(1 if int(answer.content.split()[-1].lstrip(\"(\").rstrip(\").\")) == crct_class else 0)\n",
    "        runs_results_staticdynamic.append(1 if (int(answer.content.split()[-1].lstrip(\"(\").rstrip(\").\")) in [1, 2, 3] and crct_class in [1, 2, 3]) or (int(answer.content.split()[-1].lstrip(\"(\").rstrip(\").\")) in [4, 5, 6] and crct_class in [4, 5, 6]) else 0)\n",
    "    \n",
    "    return (np.round(np.mean(runs_results), 4), np.round(np.mean(runs_results_staticdynamic), 4))\n",
    "\n",
    "\n",
    "print(list(classes.items()))\n",
    "accuracies = []\n",
    "accuracies_staticdynamic = []\n",
    "for _ in range(m_iters):\n",
    "    accuracy = evaluate_llm_fewshot(iters=iters)\n",
    "    accuracies.append(accuracy[0])\n",
    "    accuracies_staticdynamic.append(accuracy[1])\n",
    "\n",
    "print(f\"Max Accuracy over {m_iters} runs of {iters} prompts each:\", max(accuracies) * 100, \"\\nMean:\", np.round(np.mean(accuracies) * 100, 2))\n",
    "print(\"Max Accuracy for just differentiating between static and dynamic activities:\", max(accuracies_staticdynamic) * 100, \"\\nMean:\", np.round(np.mean(accuracies_staticdynamic) * 100, 2))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
