{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romit\\AppData\\Local\\Temp\\ipykernel_5696\\2757414236.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (126, 500, 3)\n",
      "Testing data shape:  (54, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "#\n",
    "#                                   ES335- Machine Learning- Assignment 1\n",
    "#\n",
    "# This file is used to create the dataset for the mini-project. The dataset is created by reading the data from\n",
    "# the Combined folder. The data is then split into training, testing, and validation sets. This split is supposed\n",
    "# to be used for all the modeling purposes.\n",
    "#\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# Constants\n",
    "time = 10\n",
    "offset = 100\n",
    "folders = [\"LAYING\", \"SITTING\", \"STANDING\", \"WALKING\", \"WALKING_DOWNSTAIRS\", \"WALKING_UPSTAIRS\"]\n",
    "classes = {\"WALKING\": 1, \"WALKING_UPSTAIRS\": 2, \"WALKING_DOWNSTAIRS\": 3, \"SITTING\": 4, \"STANDING\": 5, \"LAYING\": 6}\n",
    "\n",
    "combined_dir = os.path.join(\"Combined\")\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Train Dataset\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "dataset_dir = os.path.join(combined_dir, \"Train\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir, folder))\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, folder, file), sep=\",\", header=0)\n",
    "        df = df[offset:offset + time * 50]\n",
    "        X_train.append(df.values)\n",
    "        y_train.append(classes[folder])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Test Dataset\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "dataset_dir = os.path.join(combined_dir, \"Test\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir, folder))\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, folder, file), sep=\",\", header=0)\n",
    "        df = df[offset:offset + time * 50]\n",
    "        X_test.append(df.values)\n",
    "        y_test.append(classes[folder])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Final Dataset\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# USE THE BELOW GIVEN DATA FOR TRAINING and TESTING purposes\n",
    "\n",
    "# concatenate the training and testing data\n",
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "\n",
    "# split the data into training and testing sets. Change the seed value to obtain different random splits.\n",
    "seed = 4\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "\n",
    "print(\"Training data shape: \", X_train.shape)\n",
    "print(\"Testing data shape: \", X_test.shape)\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           accx      accy      accz  Label\n",
      "0      1.196480 -0.160690 -0.740178    2.0\n",
      "1      1.222030 -0.267295 -0.771748    2.0\n",
      "2      1.223110 -0.433863 -0.767842    2.0\n",
      "3      1.119198 -0.533790 -0.756309    2.0\n",
      "4      0.915843 -0.496126 -0.629135    2.0\n",
      "...         ...       ...       ...    ...\n",
      "62995  0.834664 -0.084789 -0.402108    2.0\n",
      "62996  0.741997 -0.107038 -0.368874    2.0\n",
      "62997  0.652846 -0.106747 -0.345863    2.0\n",
      "62998  0.640395 -0.106293 -0.339603    2.0\n",
      "62999  0.635090 -0.096517 -0.327336    2.0\n",
      "\n",
      "[63000 rows x 4 columns]\n",
      "           accx      accy      accz  Label\n",
      "0      1.293474 -0.314044 -0.248005    3.0\n",
      "1      1.258798 -0.290903 -0.513713    3.0\n",
      "2      1.258902 -0.345183 -0.505633    3.0\n",
      "3      1.235350 -0.396729 -0.239465    3.0\n",
      "4      1.259212 -0.418217 -0.006383    3.0\n",
      "...         ...       ...       ...    ...\n",
      "26995  0.929636  0.192764  0.369441    4.0\n",
      "26996  0.927869  0.191753  0.369351    4.0\n",
      "26997  0.926002  0.192286  0.366709    4.0\n",
      "26998  0.925441  0.192937  0.362487    4.0\n",
      "26999  0.928037  0.194660  0.370403    4.0\n",
      "\n",
      "[27000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "trainDataFrame = pd.DataFrame(np.concatenate((X_train.reshape((-1, 3)), np.repeat(y_train, 500).reshape(-1,1)), axis = 1), columns = [\"accx\", \"accy\", \"accz\", \"Label\"])\n",
    "print(trainDataFrame)\n",
    "trainDataFrame[\"totalacc\"] = trainDataFrame[\"accx\"]**2 + trainDataFrame[\"accy\"]**2 + trainDataFrame[\"accz\"]**2\n",
    "testDataFrame = pd.DataFrame(np.concatenate((X_test.reshape((-1, 3)), np.repeat(y_test, 500).reshape(-1,1)), axis = 1), columns = [\"accx\", \"accy\", \"accz\", \"Label\"])\n",
    "print(testDataFrame)\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# Label wise distribution of the data\n",
    "trainDataLabel1 = trainDataFrame[trainDataFrame[\"Label\"] == 1]\n",
    "trainDataLabel2 = trainDataFrame[trainDataFrame[\"Label\"] == 2]\n",
    "trainDataLabel3 = trainDataFrame[trainDataFrame[\"Label\"] == 3]\n",
    "trainDataLabel4 = trainDataFrame[trainDataFrame[\"Label\"] == 4]\n",
    "trainDataLabel5 = trainDataFrame[trainDataFrame[\"Label\"] == 5]\n",
    "trainDataLabel6 = trainDataFrame[trainDataFrame[\"Label\"] == 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model and API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq API and Models \n",
    "Groq_Token = \"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\"  # Do not share this key with anyone\n",
    "\n",
    "groq_models = {\"llama3-70b\": \"llama3-70b-8192\", \"mixtral\": \"mixtral-8x7b-32768\", \"gemma-7b\": \"gemma-7b-it\",\"llama3.1-70b\":\"llama-3.1-70b-versatile\",\"llama3-8b\":\"llama3-8b-8192\",\"llama3.1-8b\":\"llama-3.1-8b-instant\",\"gemma-9b\":\"gemma2-9b-it\"}\n",
    "modelName = \"llama3-70b\"\n",
    "llm = ChatGroq(model=groq_models[modelName], api_key=Groq_Token, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
