{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf2ed480b7015889",
   "metadata": {},
   "source": [
    "## **Task 4 : Data Collection and Decision Tree Implement  [4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d850b4d21ad3aae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T08:43:32.542505Z",
     "start_time": "2024-08-28T08:43:24.202407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Combining the training data\n",
      "Done Combining the testing data\n",
      "Done Combining the data\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "# Give the path of the test and train folder of UCI HAR Dataset\n",
    "train_path = \"./UCI HAR Dataset/train\"\n",
    "test_path = \"./UCI HAR Dataset/test\"\n",
    "\n",
    "# Dictionary of activities. Provided by the dataset.\n",
    "ACTIVITIES = {\n",
    "    1: 'WALKING',\n",
    "    2: 'WALKING_UPSTAIRS',\n",
    "    3: 'WALKING_DOWNSTAIRS',\n",
    "    4: 'SITTING',\n",
    "    5: 'STANDING',\n",
    "    6: 'LAYING',\n",
    "}\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Combining Traing Data\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# Load all the accelerometer data\n",
    "total_acc_x = pd.read_csv(os.path.join(train_path, \"Inertial Signals\", \"total_acc_x_train.txt\"), sep='\\\\s+',\n",
    "                          header=None)\n",
    "total_acc_y = pd.read_csv(os.path.join(train_path, \"Inertial Signals\", \"total_acc_y_train.txt\"), sep='\\\\s+',\n",
    "                          header=None)\n",
    "total_acc_z = pd.read_csv(os.path.join(train_path, \"Inertial Signals\", \"total_acc_z_train.txt\"), sep='\\\\s+',\n",
    "                          header=None)\n",
    "\n",
    "# Read the subject IDs\n",
    "subject_train = pd.read_csv(os.path.join(train_path, \"subject_train.txt\"), sep='\\\\s+', header=None)\n",
    "\n",
    "# Read the labels\n",
    "y = pd.read_csv(os.path.join(train_path, \"y_train.txt\"), sep='\\\\s+', header=None)\n",
    "\n",
    "# Toggle through all the subjects.\n",
    "for subject in np.unique(subject_train.values):\n",
    "\n",
    "    sub_idxs = np.where(subject_train.iloc[:, 0] == subject)[0]\n",
    "    labels = y.loc[sub_idxs]\n",
    "\n",
    "    # Toggle through all the labels.\n",
    "    for label in np.unique(labels.values):\n",
    "\n",
    "        # make the folder directory if it does not exist\n",
    "        if not os.path.exists(os.path.join(\"Combined\", \"Train\", ACTIVITIES[label])):\n",
    "            os.makedirs(os.path.join(\"Combined\", \"Train\", ACTIVITIES[label]))\n",
    "\n",
    "        label_idxs = labels[labels.iloc[:, 0] == label].index\n",
    "\n",
    "        accx = []\n",
    "        accy = []\n",
    "        accz = []\n",
    "\n",
    "        for idx in label_idxs:\n",
    "            if accx is not None:\n",
    "                accx = np.hstack((accx, total_acc_x.loc[idx][64:]))\n",
    "                accy = np.hstack((accy, total_acc_y.loc[idx][64:]))\n",
    "                accz = np.hstack((accz, total_acc_z.loc[idx][64:]))\n",
    "\n",
    "            else:\n",
    "                accx = total_acc_x.loc[idx]\n",
    "                accy = total_acc_y.loc[idx]\n",
    "                accz = total_acc_z.loc[idx]\n",
    "\n",
    "        # saving the data into csv file\n",
    "        data = pd.DataFrame({'accx': accx, 'accy': accy, 'accz': accz})\n",
    "        save_path = os.path.join(\"Combined\", \"Train\", ACTIVITIES[label], f\"Subject_{subject}.csv\")\n",
    "        data.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"Done Combining the training data\")\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Combining Test Data\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# Load all the accelerometer data\n",
    "total_acc_x = pd.read_csv(os.path.join(test_path, \"Inertial Signals\", \"total_acc_x_test.txt\"), sep='\\\\s+',\n",
    "                          header=None)\n",
    "total_acc_y = pd.read_csv(os.path.join(test_path, \"Inertial Signals\", \"total_acc_y_test.txt\"), sep='\\\\s+',\n",
    "                          header=None)\n",
    "total_acc_z = pd.read_csv(os.path.join(test_path, \"Inertial Signals\", \"total_acc_z_test.txt\"), sep='\\\\s+',\n",
    "                          header=None)\n",
    "\n",
    "# Read the subject IDs\n",
    "subject_test = pd.read_csv(os.path.join(test_path, \"subject_test.txt\"), sep='\\\\s+', header=None)\n",
    "\n",
    "# Read the labels\n",
    "y = pd.read_csv(os.path.join(test_path, \"y_test.txt\"), sep='\\\\s+', header=None)\n",
    "\n",
    "# Toggle through all the subjects.\n",
    "for subject in np.unique(subject_test.values):\n",
    "\n",
    "    sub_idxs = np.where(subject_test.iloc[:, 0] == subject)[0]\n",
    "    labels = y.loc[sub_idxs]\n",
    "\n",
    "    # Toggle through all the labels.\n",
    "    for label in np.unique(labels.values):\n",
    "\n",
    "        if not os.path.exists(os.path.join(\"Combined\", \"Test\", ACTIVITIES[label])):\n",
    "            os.makedirs(os.path.join(\"Combined\", \"Test\", ACTIVITIES[label]))\n",
    "\n",
    "        label_idxs = labels[labels.iloc[:, 0] == label].index\n",
    "\n",
    "        accx = []\n",
    "        accy = []\n",
    "        accz = []\n",
    "        for idx in label_idxs:\n",
    "            if accx is not None:\n",
    "                accx = np.hstack((accx, total_acc_x.loc[idx][64:]))\n",
    "                accy = np.hstack((accy, total_acc_y.loc[idx][64:]))\n",
    "                accz = np.hstack((accz, total_acc_z.loc[idx][64:]))\n",
    "\n",
    "            else:\n",
    "                accx = total_acc_x.loc[idx]\n",
    "                accy = total_acc_y.loc[idx]\n",
    "                accz = total_acc_z.loc[idx]\n",
    "\n",
    "        # saving the data into csv file\n",
    "        data = pd.DataFrame({'accx': accx, 'accy': accy, 'accz': accz})\n",
    "        save_path = os.path.join(\"Combined\", \"Test\", ACTIVITIES[label], f\"Subject_{subject}.csv\")\n",
    "        data.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"Done Combining the testing data\")\n",
    "print(\"Done Combining the data\")\n",
    "\n",
    "# Creation of raw dataset\n",
    "\n",
    "x = pd.read_csv(os.path.join(train_path,\"X_train.txt\"), sep='\\\\s+', header=None)\n",
    "featurel = pd.read_csv(os.path.join(\"./UCI HAR Dataset\",\"features.txt\"), sep='\\\\s+', header=None)\n",
    "featurelist = list(featurel.loc[:,1])\n",
    "x.columns = featurelist\n",
    "yg = pd.read_csv(os.path.join(train_path,\"y_train.txt\"), sep='\\\\s+', header=None)\n",
    "xtest = pd.read_csv(os.path.join(test_path,\"X_test.txt\"), sep='\\\\s+', header=None)\n",
    "xtest.columns = featurelist\n",
    "ytest = pd.read_csv(os.path.join(test_path,\"y_test.txt\"), sep='\\\\s+', header=None)\n",
    "\n",
    "# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2246b715693479a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T08:43:34.618018Z",
     "start_time": "2024-08-28T08:43:32.550164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (126, 500, 3)\n",
      "Testing data shape:  (54, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# plt.style.use('dark_background') # for dark background of plts\n",
    "\n",
    "# Constants\n",
    "time = 10\n",
    "offset = 100\n",
    "folders = [\"LAYING\", \"SITTING\", \"STANDING\", \"WALKING\", \"WALKING_DOWNSTAIRS\", \"WALKING_UPSTAIRS\"]\n",
    "classes = {\"WALKING\": 1, \"WALKING_UPSTAIRS\": 2, \"WALKING_DOWNSTAIRS\": 3, \"SITTING\": 4, \"STANDING\": 5, \"LAYING\": 6}\n",
    "\n",
    "combined_dir = os.path.join(\"Combined\")\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Train Dataset\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "dataset_dir = os.path.join(combined_dir, \"Train\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir, folder))\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, folder, file), sep=\",\", header=0)\n",
    "        df = df[offset:offset + time * 50]\n",
    "        X_train.append(df.values)\n",
    "        y_train.append(classes[folder])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Test Dataset\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "dataset_dir = os.path.join(combined_dir, \"Test\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir, folder))\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(dataset_dir, folder, file), sep=\",\", header=0)\n",
    "        df = df[offset:offset + time * 50]\n",
    "        X_test.append(df.values)\n",
    "        y_test.append(classes[folder])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Final Dataset\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# USE THE BELOW GIVEN DATA FOR TRAINING and TESTING purposes\n",
    "\n",
    "# concatenate the training and testing data\n",
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "\n",
    "# split the data into training and testing sets. Change the seed value to obtain different random splits.\n",
    "seed = 4\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "\n",
    "print(\"Training data shape: \", X_train.shape)\n",
    "print(\"Testing data shape: \", X_test.shape)\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c877c015027e7343",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T08:43:35.398619Z",
     "start_time": "2024-08-28T08:43:35.379202Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.concatenate((X_train.reshape((-1, 3)), np.repeat(y_train, 500).reshape(-1,1)), axis = 1), columns = [\"accx\", \"accy\", \"accz\", \"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f0cec4a11af08516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T08:43:35.467025Z",
     "start_time": "2024-08-28T08:43:35.458090Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"totalacc\"] = df[\"accx\"]**2 + df[\"accy\"]**2 + df[\"accz\"]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e1a476919367d64c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T08:43:39.729858Z",
     "start_time": "2024-08-28T08:43:35.531328Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 0% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='0'\n",
       "                  max='126',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  0\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature extraction finished ***\n"
     ]
    }
   ],
   "source": [
    "import tsfel\n",
    "\n",
    "cfg = tsfel.get_features_by_domain()\n",
    "features = tsfel.time_series_features_extractor(cfg,df[[\"accx\",\"accy\",\"accz\"]], fs=50, window_size=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10768a28",
   "metadata": {},
   "source": [
    "### 1. Use the Decision Tree model trained on the UCI-HAR dataset to predict the activities that you performed. Report the accuracy, precision, recall and confusion matrix of the model. You have three version of UCI dataset you can use a)Raw data from accelerometer, b)TSFEL featurised data, c)Features provided by author. Choose which version to use, ensuring that your test data is similar to your training data. How did the model perform? [1 marks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cbb4e6b339ec0854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T08:43:40.040119Z",
     "start_time": "2024-08-28T08:43:39.810613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of author's raw data trained decision tree on our data is 38.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "d = pd.read_csv(\"./Task4-Data/Processed/Combined.csv\")\n",
    "acc = d[['gFx','gFy','gFz']]\n",
    "dtest = d[\"Label\"]\n",
    "X_train_reshaped = X_train.reshape(-1,1500)\n",
    "acc = acc.to_numpy()\n",
    "acc_reshaped = acc.reshape(18,1500)\n",
    "clf = DecisionTreeClassifier(random_state=2,max_depth=6)\n",
    "clf.fit(X_train_reshaped, y_train)\n",
    "dpred = clf.predict(acc_reshaped)\n",
    "print(f\"Accuracy of author's raw data trained decision tree on our data is {accuracy_score(dtest[::500], dpred)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b507be1aeddd0e16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T08:43:43.889482Z",
     "start_time": "2024-08-28T08:43:40.103565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 100% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='18'\n",
       "                  max='18',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  18\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature extraction finished ***\n",
      "Accuracy of TSFEL trained decision tree on our data is 27.78%\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=2,max_depth=8)\n",
    "\n",
    "clf.fit(features, y_train)\n",
    "# print(d.columns)\n",
    "d.columns = ['Person','accx','accy','accz','Total acc','Label']\n",
    "dtsfel = tsfel.time_series_features_extractor(cfg,d[[\"accx\",\"accy\",\"accz\"]], fs=50, window_size=500)\n",
    "dpred = clf.predict(dtsfel)\n",
    "print(f\"Accuracy of TSFEL trained decision tree on our data is {accuracy_score(dtest[::500],dpred)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd25f9",
   "metadata": {},
   "source": [
    "> We cant use author's featurized data as we dont have the same features of data from our recordings.\n",
    "\n",
    "| Method | Accuracy | \n",
    "| --- | --- |\n",
    "| Raw Data | 38.89% |\n",
    "| TSFEL | 27.78% | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243cd546",
   "metadata": {},
   "source": [
    "### 2. Use the data you collected to predict the activities that you performed. Decide whether to apply preprocessing and featurization, and if so, choose the appropriate methods. How did the model perform? [1 marks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a9dae8",
   "metadata": {},
   "source": [
    "`own data to train and test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33cd03a6d16e74ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T09:05:58.065310Z",
     "start_time": "2024-08-28T09:05:58.015749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model trained and tested on our raw data is 80.44%\n"
     ]
    }
   ],
   "source": [
    "# raw data first\n",
    "X_train,x_test,y_train,y_test = train_test_split(acc,dtest,test_size=0.3)\n",
    "clf = DecisionTreeClassifier(random_state=2,max_depth=8)\n",
    "clf.fit(X_train,y_train)\n",
    "dpred = clf.predict(x_test)\n",
    "print(f\"Accuracy of the model trained and tested on our raw data is {accuracy_score(y_test,dpred)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecada1f",
   "metadata": {},
   "source": [
    "To reduce bias in the training data, a better approach will be to train the model with 2 person's data and test it on 3rd person's data\n",
    "\n",
    "> Now applying `featurization` using **tsfel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43aef6b061a6330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T09:06:06.698427Z",
     "start_time": "2024-08-28T09:06:03.237497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 100% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='18'\n",
       "                  max='18',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  18\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature extraction finished ***\n"
     ]
    }
   ],
   "source": [
    "cfg = tsfel.get_features_by_domain()\n",
    "acctsfel = tsfel.time_series_features_extractor(cfg,acc,fs=50,window_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4dce94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model trained and tested on our raw data is 50.00%\n"
     ]
    }
   ],
   "source": [
    "xtr,xt,ytr,yt = train_test_split(acctsfel,dtest[::500],test_size=0.3)\n",
    "clf = DecisionTreeClassifier(random_state=42,max_depth=8)\n",
    "clf.fit(xtr,ytr)\n",
    "ypr = clf.predict(xt)\n",
    "print(f\"Accuracy of the model trained and tested on our raw data is {accuracy_score(yt,ypr)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d38e6e7",
   "metadata": {},
   "source": [
    "Accuracy fell after featurizing the data using tsfel\n",
    "| Method | Accuracy | \n",
    "| --- | --- |\n",
    "| Raw Data | 80.44% |\n",
    "| TSFEL | 50% | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e6984",
   "metadata": {},
   "source": [
    "### 3. Use the Few-Shot prompting method using UCI-HAR dataset to predict the activities that you performed. Ensure that both your examples and test query undergo similar preprocessing. How did the model perform? [1 marks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec958b1",
   "metadata": {},
   "source": [
    "for _giving example_ data to the `LLM`, we'll use the tsfel data extracted from the raw UCI-HAR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e121012",
   "metadata": {},
   "source": [
    "To choose which features should be used, we'll ask the llm itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4df9576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq API and Models \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "load_dotenv()\n",
    "groq_api_key_akash = os.getenv(\"API_KEY_AKASH\") #loading api keys from .env file \n",
    "groq_api_key_romit= os.getenv(\"API_KEY_ROMIT\")\n",
    "\n",
    "groq_models = {\n",
    "    \"llama3-70b\": \"llama3-70b-8192\", \n",
    "    \"mixtral\": \"mixtral-8x7b-32768\", \n",
    "    \"gemma-7b\": \"gemma-7b-it\",\n",
    "    \"llama3.1-70b\":\"llama-3.1-70b-versatile\",\n",
    "    \"llama3-8b\":\"llama3-8b-8192\",\n",
    "    \"llama3.1-8b\":\"llama-3.1-8b-instant\",\n",
    "    \"gemma-9b\":\"gemma2-9b-it\"\n",
    "}\n",
    "\n",
    "modelName = \"llama3.1-70b\"\n",
    "\n",
    "llm = ChatGroq(model=groq_models[modelName], api_key=groq_api_key_akash, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "01159861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a Human Activity Recognition (HAR) model, I'd be delighted to extract and utilize the most informative features from accelerometer data to accurately identify and classify various human activities. Here are the top features I'd select from the Time Series Feature Extraction Library (TSFEL) to achieve this:\n",
      "\n",
      "**1. Time Domain Features:**\n",
      "\n",
      "* **Mean** (`mean`): Average acceleration value, which can help distinguish between activities with different intensity levels (e.g., walking vs. running).\n",
      "* **Standard Deviation** (`std`): Measures the variability of acceleration values, useful for identifying activities with varying movement patterns (e.g., jumping vs. walking).\n",
      "* **Root Mean Square** (`rms`): A measure of the average power of the signal, which can help distinguish between activities with different energy expenditure (e.g., running vs. sitting).\n",
      "\n",
      "**2. Frequency Domain Features:**\n",
      "\n",
      "* **Spectral Power** (`spectral_power`): Represents the distribution of power across different frequency bands, which can help identify activities with distinct frequency patterns (e.g., walking vs. cycling).\n",
      "* **Peak Frequency** (`peak_frequency`): The frequency at which the signal has the highest power, which can help distinguish between activities with different movement patterns (e.g., running vs. jumping).\n",
      "\n",
      "**3. Time-Frequency Domain Features:**\n",
      "\n",
      "* **Short-Time Fourier Transform (STFT) Energy** (`stft_energy`): A measure of the energy distribution across time and frequency, which can help identify activities with varying movement patterns and frequency content (e.g., dancing vs. walking).\n",
      "* **Continuous Wavelet Transform (CWT) Coefficients** (`cwt_coefficients`): Represent the signal's time-frequency representation, which can help capture activities with complex movement patterns (e.g., yoga vs. walking).\n",
      "\n",
      "**4. Non-Linear Features:**\n",
      "\n",
      "* **Approximate Entropy** (`approx_entropy`): Measures the complexity or irregularity of the signal, which can help distinguish between activities with different movement patterns (e.g., running vs. walking).\n",
      "* **Sample Entropy** (`sample_entropy`): A measure of the signal's randomness, which can help identify activities with varying movement patterns (e.g., jumping vs. walking).\n",
      "\n",
      "**5. Other Features:**\n",
      "\n",
      "* **Autocorrelation** (`autocorrelation`): Measures the similarity between the signal and its time-shifted version, which can help identify activities with periodic movement patterns (e.g., walking vs. running).\n",
      "* **Cross-Correlation** (`cross_correlation`): Measures the similarity between two signals, which can help identify activities with coordinated movement patterns (e.g., cycling vs. walking).\n",
      "\n",
      "These features are widely used in HAR research and have been shown to be effective in distinguishing between various human activities. By extracting and combining these features, I can develop a robust and accurate HAR model.\n",
      "\n",
      "Here's a sample Python code snippet using TSFEL to extract these features from accelerometer data:\n",
      "```python\n",
      "import pandas as pd\n",
      "from tsfel import Tsfel\n",
      "\n",
      "# Load accelerometer data\n",
      "data = pd.read_csv('accelerometer_data.csv')\n",
      "\n",
      "# Create a TSFEL object\n",
      "tsfel = Tsfel()\n",
      "\n",
      "# Extract features\n",
      "features = tsfel.extract_features(data, \n",
      "                                 features_to_extract=['mean', 'std', 'rms', \n",
      "                                                     'spectral_power', 'peak_frequency', \n",
      "                                                     'stft_energy', 'cwt_coefficients', \n",
      "                                                     'approx_entropy', 'sample_entropy', \n",
      "                                                     'autocorrelation', 'cross_correlation'])\n",
      "\n",
      "# Convert features to a Pandas DataFrame\n",
      "features_df = pd.DataFrame(features)\n",
      "\n",
      "# Use the extracted features for HAR model development\n",
      "```\n",
      "Note that the specific features and their combinations may vary depending on the dataset, experimental setup, and the activities being recognized.\n"
     ]
    }
   ],
   "source": [
    "answer = llm.invoke(\"If you are a Human Activity recognition model. What features will be best suited to identify and classify data as an activity. Select and name the specific tsfel features from the features extracted from a raw dataset containing accelerometer data for Human Activity recognition\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc722c1b",
   "metadata": {},
   "source": [
    "Hence, we chose the following features `\"accx_Spectral entropy\", \"accx_Power bandwidth\", \"accx_Maximum frequency\", \"accx_Median frequency\", \"accx_Root mean square\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "480b9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_chosen = [\"accx_Spectral entropy\", \"accx_Power bandwidth\", \"accx_Maximum frequency\", \"accx_Median frequency\", \"accx_Root mean square\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6148d78c",
   "metadata": {},
   "source": [
    "Next, we choose: `\"accx_Mean\", \"accx_Standard deviation\", \"accy_Mean\", \"accy_Standard deviation\", \"accz_Mean\", \"accz_Standard deviation\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9e068c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_chosen = [\"accx_Mean\", \"accx_Standard deviation\", \"accy_Mean\", \"accy_Standard deviation\", \"accz_Mean\", \"accz_Standard deviation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3ee69abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accx_Mean</th>\n",
       "      <th>accx_Standard deviation</th>\n",
       "      <th>accy_Mean</th>\n",
       "      <th>accy_Standard deviation</th>\n",
       "      <th>accz_Mean</th>\n",
       "      <th>accz_Standard deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.902649</td>\n",
       "      <td>0.203626</td>\n",
       "      <td>-0.282842</td>\n",
       "      <td>0.146770</td>\n",
       "      <td>-0.412284</td>\n",
       "      <td>0.147509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.255891</td>\n",
       "      <td>0.007540</td>\n",
       "      <td>0.644078</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.732082</td>\n",
       "      <td>0.007904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.978388</td>\n",
       "      <td>0.003478</td>\n",
       "      <td>-0.179956</td>\n",
       "      <td>0.008010</td>\n",
       "      <td>-0.279538</td>\n",
       "      <td>0.006517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.951163</td>\n",
       "      <td>0.222022</td>\n",
       "      <td>-0.311365</td>\n",
       "      <td>0.124206</td>\n",
       "      <td>-0.245482</td>\n",
       "      <td>0.127796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.001998</td>\n",
       "      <td>0.238608</td>\n",
       "      <td>-0.188667</td>\n",
       "      <td>0.147697</td>\n",
       "      <td>-0.141150</td>\n",
       "      <td>0.127582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.977764</td>\n",
       "      <td>0.198636</td>\n",
       "      <td>-0.199177</td>\n",
       "      <td>0.131601</td>\n",
       "      <td>-0.267679</td>\n",
       "      <td>0.151983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.822427</td>\n",
       "      <td>0.073545</td>\n",
       "      <td>0.354603</td>\n",
       "      <td>0.114741</td>\n",
       "      <td>0.429520</td>\n",
       "      <td>0.058798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.188234</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.533758</td>\n",
       "      <td>0.019640</td>\n",
       "      <td>0.833852</td>\n",
       "      <td>0.013331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.904258</td>\n",
       "      <td>0.206619</td>\n",
       "      <td>-0.377194</td>\n",
       "      <td>0.175782</td>\n",
       "      <td>-0.308029</td>\n",
       "      <td>0.174014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.922667</td>\n",
       "      <td>0.291077</td>\n",
       "      <td>-0.172923</td>\n",
       "      <td>0.149556</td>\n",
       "      <td>-0.397550</td>\n",
       "      <td>0.168488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     accx_Mean  accx_Standard deviation  accy_Mean  accy_Standard deviation  \\\n",
       "0     0.902649                 0.203626  -0.282842                 0.146770   \n",
       "1     0.255891                 0.007540   0.644078                 0.004683   \n",
       "2     0.978388                 0.003478  -0.179956                 0.008010   \n",
       "3     0.951163                 0.222022  -0.311365                 0.124206   \n",
       "4     1.001998                 0.238608  -0.188667                 0.147697   \n",
       "..         ...                      ...        ...                      ...   \n",
       "121   0.977764                 0.198636  -0.199177                 0.131601   \n",
       "122   0.822427                 0.073545   0.354603                 0.114741   \n",
       "123   0.188234                 0.004684   0.533758                 0.019640   \n",
       "124   0.904258                 0.206619  -0.377194                 0.175782   \n",
       "125   0.922667                 0.291077  -0.172923                 0.149556   \n",
       "\n",
       "     accz_Mean  accz_Standard deviation  \n",
       "0    -0.412284                 0.147509  \n",
       "1     0.732082                 0.007904  \n",
       "2    -0.279538                 0.006517  \n",
       "3    -0.245482                 0.127796  \n",
       "4    -0.141150                 0.127582  \n",
       "..         ...                      ...  \n",
       "121  -0.267679                 0.151983  \n",
       "122   0.429520                 0.058798  \n",
       "123   0.833852                 0.013331  \n",
       "124  -0.308029                 0.174014  \n",
       "125  -0.397550                 0.168488  \n",
       "\n",
       "[126 rows x 6 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_subset = features[features_chosen]\n",
    "features_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74590251",
   "metadata": {},
   "source": [
    "for _asking answers_ to the `LLM`, we'll use the tsfel data extracted from the **data collected by us**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8768ad56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accx_Mean</th>\n",
       "      <th>accx_Standard deviation</th>\n",
       "      <th>accy_Mean</th>\n",
       "      <th>accy_Standard deviation</th>\n",
       "      <th>accz_Mean</th>\n",
       "      <th>accz_Standard deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.987310</td>\n",
       "      <td>0.171184</td>\n",
       "      <td>-0.014573</td>\n",
       "      <td>0.212459</td>\n",
       "      <td>-0.157483</td>\n",
       "      <td>0.177988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.937482</td>\n",
       "      <td>0.256595</td>\n",
       "      <td>-0.022257</td>\n",
       "      <td>0.211054</td>\n",
       "      <td>-0.330475</td>\n",
       "      <td>0.212309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.988906</td>\n",
       "      <td>0.372105</td>\n",
       "      <td>-0.068037</td>\n",
       "      <td>0.203122</td>\n",
       "      <td>-0.026799</td>\n",
       "      <td>0.246365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.996646</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>0.004232</td>\n",
       "      <td>-0.075714</td>\n",
       "      <td>0.009085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.984541</td>\n",
       "      <td>0.053810</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>0.008248</td>\n",
       "      <td>-0.151944</td>\n",
       "      <td>0.072393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.156439</td>\n",
       "      <td>0.074329</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>0.008746</td>\n",
       "      <td>0.980034</td>\n",
       "      <td>0.089981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000672</td>\n",
       "      <td>0.211837</td>\n",
       "      <td>-0.004624</td>\n",
       "      <td>0.194552</td>\n",
       "      <td>-0.081874</td>\n",
       "      <td>0.184651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.953038</td>\n",
       "      <td>0.245485</td>\n",
       "      <td>0.079508</td>\n",
       "      <td>0.145019</td>\n",
       "      <td>-0.279726</td>\n",
       "      <td>0.172761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.989491</td>\n",
       "      <td>0.531527</td>\n",
       "      <td>0.039558</td>\n",
       "      <td>0.159744</td>\n",
       "      <td>0.010351</td>\n",
       "      <td>0.238845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.995629</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>-0.059680</td>\n",
       "      <td>0.004350</td>\n",
       "      <td>0.096705</td>\n",
       "      <td>0.007608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000318</td>\n",
       "      <td>0.005671</td>\n",
       "      <td>-0.004627</td>\n",
       "      <td>0.006555</td>\n",
       "      <td>-0.033433</td>\n",
       "      <td>0.007823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.074808</td>\n",
       "      <td>0.027697</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>0.993039</td>\n",
       "      <td>0.058901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.952970</td>\n",
       "      <td>0.328076</td>\n",
       "      <td>-0.041552</td>\n",
       "      <td>0.167619</td>\n",
       "      <td>-0.280810</td>\n",
       "      <td>0.219832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.958720</td>\n",
       "      <td>0.270426</td>\n",
       "      <td>-0.008050</td>\n",
       "      <td>0.198510</td>\n",
       "      <td>-0.264832</td>\n",
       "      <td>0.162773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000518</td>\n",
       "      <td>0.362457</td>\n",
       "      <td>-0.007730</td>\n",
       "      <td>0.184604</td>\n",
       "      <td>-0.054473</td>\n",
       "      <td>0.211944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.981320</td>\n",
       "      <td>0.053387</td>\n",
       "      <td>-0.049646</td>\n",
       "      <td>0.015568</td>\n",
       "      <td>-0.206886</td>\n",
       "      <td>0.026359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.986321</td>\n",
       "      <td>0.044506</td>\n",
       "      <td>-0.008074</td>\n",
       "      <td>0.005362</td>\n",
       "      <td>-0.156089</td>\n",
       "      <td>0.052144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.001347</td>\n",
       "      <td>0.005226</td>\n",
       "      <td>-0.041702</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.998008</td>\n",
       "      <td>0.003265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accx_Mean  accx_Standard deviation  accy_Mean  accy_Standard deviation  \\\n",
       "0    0.987310                 0.171184  -0.014573                 0.212459   \n",
       "1    0.937482                 0.256595  -0.022257                 0.211054   \n",
       "2    0.988906                 0.372105  -0.068037                 0.203122   \n",
       "3    0.996646                 0.003538   0.032597                 0.004232   \n",
       "4    0.984541                 0.053810   0.010729                 0.008248   \n",
       "5    0.156439                 0.074329  -0.004598                 0.008746   \n",
       "6    1.000672                 0.211837  -0.004624                 0.194552   \n",
       "7    0.953038                 0.245485   0.079508                 0.145019   \n",
       "8    0.989491                 0.531527   0.039558                 0.159744   \n",
       "9    0.995629                 0.003896  -0.059680                 0.004350   \n",
       "10   1.000318                 0.005671  -0.004627                 0.006555   \n",
       "11   0.074808                 0.027697   0.000839                 0.004059   \n",
       "12   0.952970                 0.328076  -0.041552                 0.167619   \n",
       "13   0.958720                 0.270426  -0.008050                 0.198510   \n",
       "14   1.000518                 0.362457  -0.007730                 0.184604   \n",
       "15   0.981320                 0.053387  -0.049646                 0.015568   \n",
       "16   0.986321                 0.044506  -0.008074                 0.005362   \n",
       "17  -0.001347                 0.005226  -0.041702                 0.002309   \n",
       "\n",
       "    accz_Mean  accz_Standard deviation  \n",
       "0   -0.157483                 0.177988  \n",
       "1   -0.330475                 0.212309  \n",
       "2   -0.026799                 0.246365  \n",
       "3   -0.075714                 0.009085  \n",
       "4   -0.151944                 0.072393  \n",
       "5    0.980034                 0.089981  \n",
       "6   -0.081874                 0.184651  \n",
       "7   -0.279726                 0.172761  \n",
       "8    0.010351                 0.238845  \n",
       "9    0.096705                 0.007608  \n",
       "10  -0.033433                 0.007823  \n",
       "11   0.993039                 0.058901  \n",
       "12  -0.280810                 0.219832  \n",
       "13  -0.264832                 0.162773  \n",
       "14  -0.054473                 0.211944  \n",
       "15  -0.206886                 0.026359  \n",
       "16  -0.156089                 0.052144  \n",
       "17   0.998008                 0.003265  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_subset_test = dtsfel[features_chosen]\n",
    "features_subset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "76c653b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query_fewshot(train, test):\n",
    "    return f\"\"\"\n",
    "    * You are a Human Activity Recognition model. \n",
    "    * Your task is to classify the given data as one of the six activity classes listed below.\n",
    "    * The data is recorded by accelerometer positioned above the torso of the person.\n",
    "    * The data's format is as such that each line contains the \n",
    "    -> Mean acceleration in X, Standard deviation in acceleration in X, Mean acceleration in Y, Standard deviation in acceleration in Y, Mean acceleration in Z, Standard deviation in acceleration in Z.\n",
    "    * Features are extracted from accelorometer data using tsfel.\n",
    "    * Pay attention to the examples given.\n",
    "\n",
    "    1. Walking\n",
    "    2. Walking Upstairs\n",
    "    3. Walking Downstairs\n",
    "    4. Sitting\n",
    "    5. Standing\n",
    "    6. Lying Down\n",
    "\n",
    "    *Here are few examples:\n",
    "    {train}\n",
    "\n",
    "    * Provide JUST the number corresponding to the predicted activity\n",
    "\n",
    "    data: \n",
    "    {test}\n",
    "    \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a9f92dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Accuracy over 5 runs of 10 prompts each: 70.0 \n",
      "Mean: 28.0\n",
      "Max Accuracy for just differentiating between static and dynamic activities: 100.0 \n",
      "Mean: 60.0\n"
     ]
    }
   ],
   "source": [
    "m_iters = 5\n",
    "iters = 10\n",
    "number_of_examples = 6\n",
    "number_of_features = 6\n",
    "import random\n",
    "\n",
    "def evaluate_llm_fewshot(iters = 10):\n",
    "    runs_results = []\n",
    "    runs_results_staticdynamic = []\n",
    "    \n",
    "    train_string = \"\"\n",
    "\n",
    "    for i in range(number_of_examples):\n",
    "        c = (i % 6) + 1  # class number\n",
    "        sample = random.randint(0, 20) # 21 is length of a training dataframe of a specific activity\n",
    "        train_string += str(i+1) + \". Data:\\n\" + \", \".join([str(point) for point in features_subset.iloc[((c-1)*21) + sample].to_numpy()]) + \"\\n\\nActivity: \" + str(c) + \": \" + ACTIVITIES[c] + \"\\n\\n\"\n",
    "\n",
    "    for _ in range(iters):\n",
    "        crct_class = random.randint(1, 6)\n",
    "        sample = random.randint(0, 2) # 3 persons data was recorder\n",
    "        test_data = features_subset_test.iloc[(sample*3) + crct_class - 1]\n",
    "        test_string = \", \".join([str(point) for point in test_data.to_numpy()])\n",
    "        query = build_query_fewshot(train_string, test_string)\n",
    "        \n",
    "        answer = llm.invoke(query)\n",
    "\n",
    "        # print(answer.content, crct_class)\n",
    "        pred_digit = int(answer.content.split()[-1].lstrip(\"(\").rstrip(\").\"))\n",
    "        runs_results.append(1 if pred_digit == crct_class else 0)\n",
    "        runs_results_staticdynamic.append(1 if (pred_digit in [1, 2, 3] and crct_class in [1, 2, 3]) or (pred_digit in [4, 5, 6] and crct_class in [4, 5, 6]) else 0)\n",
    "    \n",
    "    return (np.round(np.mean(runs_results), 4), np.round(np.mean(runs_results_staticdynamic), 4))\n",
    "\n",
    "accuracies = []\n",
    "accuracies_staticdynamic = []\n",
    "for _ in range(m_iters):\n",
    "    accuracy = evaluate_llm_fewshot(iters=iters)\n",
    "    accuracies.append(accuracy[0])\n",
    "    accuracies_staticdynamic.append(accuracy[1])\n",
    "\n",
    "print(f\"Max Accuracy over {m_iters} runs of {iters} prompts each:\", max(accuracies) * 100, \"\\nMean:\", np.round(np.mean(accuracies) * 100, 2))\n",
    "print(\"Max Accuracy for just differentiating between static and dynamic activities:\", max(accuracies_staticdynamic) * 100, \"\\nMean:\", np.round(np.mean(accuracies_staticdynamic) * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc69553",
   "metadata": {},
   "source": [
    "> Experimenting with the features: `\"accx_Spectral entropy\", \"accx_Power bandwidth\", \"accx_Maximum frequency\", \"accx_Median frequency\", \"accx_Root mean square\"`  \n",
    "> We got accuracy of `15%`\n",
    "\n",
    "> Experimenting with the features: `\"accx_Mean\", \"accx_Standard deviation\", \"accy_Mean\", \"accy_Standard deviation\", \"accz_Mean\", \"accz_Standard deviation\"`  \n",
    "> We got **max** accuracy of `70%` and **mean** of `28%`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3592bae7",
   "metadata": {},
   "source": [
    "### 4. Use the Few-Shot prompting method using the data you collected to predict the activities that you performed. Adopt proper processing methods as needed. How did the model perform? [1 marks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2563be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the whole dataset in this case is last case's test set\n",
    "dataset = features_subset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_iters = 5\n",
    "iters = 10\n",
    "number_of_examples = 6\n",
    "number_of_features = 6\n",
    "import random\n",
    "\n",
    "def evaluate_llm_fewshot(iters = 10):\n",
    "    runs_results = []\n",
    "    runs_results_staticdynamic = []\n",
    "\n",
    "    test_person = random.randint(0, 2)\n",
    "    train_people = [i for i in [0, 1, 2] if i != test_person]\n",
    "    features_subset_test = dataset.iloc[test_person*6:test_person*6 + 6]\n",
    "    features_subset = pd.concat([dataset.iloc[train_people[0]*6:train_people[0]*6 + 6], dataset.iloc[train_people[1]*6:train_people[1]*6 + 6]])\n",
    "    # print(features_subset)\n",
    "    # print(features_subset_test)\n",
    "\n",
    "    train_string = \"\"\n",
    "    for i in range(number_of_examples):\n",
    "        c = (i % 6) + 1  # class number\n",
    "        sample = random.randint(0, 1) # 2 is length of a training dataframe of a specific activity\n",
    "        train_string += str(i+1) + \". Data:\\n\" + \", \".join([str(point) for point in features_subset.iloc[(sample*3) + c - 1].to_numpy()]) + \"\\n\\nActivity: \" + str(c) + \": \" + ACTIVITIES[c] + \"\\n\\n\"\n",
    "\n",
    "    for _ in range(iters):\n",
    "        crct_class = random.randint(1, 6)\n",
    "        test_data = features_subset_test.iloc[crct_class - 1]\n",
    "        test_string = \", \".join([str(point) for point in test_data.to_numpy()])\n",
    "        query = build_query_fewshot(train_string, test_string)\n",
    "        # print(query)\n",
    "        \n",
    "        answer = llm.invoke(query)\n",
    "\n",
    "        # print(answer.content, crct_class)\n",
    "        pred_digit = int(answer.content.split()[-1].lstrip(\"(\").rstrip(\").\"))\n",
    "        runs_results.append(1 if pred_digit == crct_class else 0)\n",
    "        runs_results_staticdynamic.append(1 if (pred_digit in [1, 2, 3] and crct_class in [1, 2, 3]) or (pred_digit in [4, 5, 6] and crct_class in [4, 5, 6]) else 0)\n",
    "    \n",
    "    return (np.round(np.mean(runs_results), 4), np.round(np.mean(runs_results_staticdynamic), 4))\n",
    "\n",
    "accuracies = []\n",
    "accuracies_staticdynamic = []\n",
    "for _ in range(m_iters):\n",
    "    accuracy = evaluate_llm_fewshot(iters=iters)\n",
    "    accuracies.append(accuracy[0])\n",
    "    accuracies_staticdynamic.append(accuracy[1])\n",
    "\n",
    "print(f\"Max Accuracy over {m_iters} runs of {iters} prompts each:\", max(accuracies) * 100, \"\\nMean:\", np.round(np.mean(accuracies) * 100, 2))\n",
    "print(\"Max Accuracy for just differentiating between static and dynamic activities:\", max(accuracies_staticdynamic) * 100, \"\\nMean:\", np.round(np.mean(accuracies_staticdynamic) * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12b428b",
   "metadata": {},
   "source": [
    "> Experimenting with the features: `\"accx_Mean\", \"accx_Standard deviation\", \"accy_Mean\", \"accy_Standard deviation\", \"accz_Mean\", \"accz_Standard deviation\"`  \n",
    "> We got **max** accuracy of `70%` and **mean** of `48%`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
